{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.network import VGGModel\n",
    "from src.dataset import MNISTDataset\n",
    "from src.utils import train, evaluate, evaluate_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model backbone\n",
    "device = \"cpu\"\n",
    "vgg16 = VGGModel(num_classes=10, device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg16.model.classifier.parameters(), lr=1e-4)  # Only train classifier\n",
    "\n",
    "# Dataset\n",
    "dataset = MNISTDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment/Uncomment if you do not want/want to train the model\n",
    "for e in range(10):\n",
    "      print(f\"Epoch {e}...\")\n",
    "      train(vgg16.model,\n",
    "            dataset.train_loader,\n",
    "            criterion, \n",
    "            optimizer, \n",
    "            device)\n",
    "      evaluate(vgg16.model,\n",
    "               dataset.test_loader,\n",
    "               device)\n",
    "      \n",
    "# Save the trained model\n",
    "torch.save(vgg16.model.state_dict(), \"./models/vgg16_mnist.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running the explainers, create the result folders\n",
    "import os\n",
    "\n",
    "folders = ['./results/MNIST/LIME/images/', \n",
    "           './results/MNIST/SHAP/images/', \n",
    "           './results/MNIST/LRP/images/']\n",
    "\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(f\"Created: {folder}\")\n",
    "    else:\n",
    "        print(f\"Already exists: {folder}\")\n",
    "        \n",
    "# Load a pre-trained model, if available\n",
    "vgg16.model.load_state_dict(torch.load(\"./models/vgg16_mnist.pth\", weights_only=True))\n",
    "vgg16.model.eval()\n",
    "\n",
    "# Check the accuracy on the test set\n",
    "evaluate_per_class(vgg16.model,\n",
    "                    dataset.test_loader,\n",
    "                    device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "\n",
    "#Â size of the image for the model\n",
    "def to_model_size(x):\n",
    "    if x.ndim == 3:\n",
    "        return torch.tensor(x)[None, :, :, :].permute(0, 3, 1, 2) \n",
    "    return torch.tensor(x).permute(0, 3, 1, 2)  # numpy to torch\n",
    "\n",
    "def to_explainer_size(x):\n",
    "    return x[0].permute(1, 2, 0).cpu().numpy()  # torch to numpy\n",
    "\n",
    "# Wrap the model\n",
    "def predict_fn(x):\n",
    "    \"\"\"Works with numpy images for compatibility with LIME\"\"\"\n",
    "    global vgg16, to_explainer_size\n",
    "    x_model = to_model_size(x).to(device)  # numpy to torch\n",
    "    probs = vgg16.model(x_model)\n",
    "    probs_numpy = probs.detach().cpu().numpy()\n",
    "    return probs_numpy\n",
    "\n",
    "# Pick images from the test set and compute the LIME explanation\n",
    "reductions = []\n",
    "for idx in range(100):\n",
    "    print(f\"Recursive explanation for Image {idx}...\")\n",
    "    # 1. Pick a random image and store input, label, and prediction\n",
    "    image, y = dataset.test_dataset[idx]\n",
    "    image = image[None, :, :, :].to(device)  # expand dims and send to device\n",
    "    \n",
    "    prediction = int(torch.argmax(vgg16.model(image)))\n",
    "    predictions = [prediction]\n",
    "    correctness = (y == prediction)\n",
    "\n",
    "    # 2. Loop to find the fixed point\n",
    "    iteration = 0\n",
    "    prev_image_rgb_first = None\n",
    "    while True:\n",
    "        # Convert grayscale to RGB for LIME\n",
    "        image_rgb = np.stack([image[0, 0, :, :]] * 3, axis=0)\n",
    "        image_rgb = np.moveaxis(image_rgb, 0, -1)\n",
    "        \n",
    "        prediction = vgg16.model(torch.tensor(image_rgb).permute(2, 1, 0)[None, :, :, :])\n",
    "        predictions.append(int(torch.argmax(prediction)))\n",
    "\n",
    "        explanation = explainer.explain_instance(\n",
    "            image_rgb,\n",
    "            classifier_fn=predict_fn,\n",
    "            top_labels=5,\n",
    "            hide_color=0,\n",
    "            num_samples=1000,\n",
    "            progress_bar=False\n",
    "        )\n",
    "\n",
    "        temp, mask = explanation.get_image_and_mask(\n",
    "            explanation.top_labels[0],\n",
    "            positive_only=True,\n",
    "            negative_only=False,\n",
    "            num_features=50,\n",
    "            hide_rest=True\n",
    "        )\n",
    "\n",
    "        # Ensure float values are in [0,1]\n",
    "        img_boundry2 = mark_boundaries(temp.astype(np.float32), mask)\n",
    "\n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(8, 4))\n",
    "        axes[0].imshow(image_rgb, cmap='gray')\n",
    "        axes[0].set_title(\"Input\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(img_boundry2, cmap='gray')\n",
    "        axes[1].set_title(\"Boundary\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        image_rgb_first = image_rgb[:,:,0].flatten()\n",
    "        mask = mask.flatten()\n",
    "        image_rgb_first[mask > 0] = 0.\n",
    "        image_rgb_first = image_rgb_first.reshape((32, 32))\n",
    "        \n",
    "        axes[2].imshow(image_rgb_first, cmap='gray')\n",
    "        axes[2].set_title(\"Fixed Point Iteration\")\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"./results/MNIST/LIME/images/image-{idx}-recursive-iteration{iteration}.png\")\n",
    "        iteration += 1\n",
    "        \n",
    "        if np.equal(image_rgb_first, prev_image_rgb_first).all() and prev_image_rgb_first is not None:\n",
    "            break\n",
    "        prev_image_rgb_first = image_rgb_first\n",
    "        image = image_rgb_first[None, None, :, :]\n",
    "    \n",
    "    image_flatten = dataset.test_dataset[idx][0].detach().numpy().flatten()\n",
    "    non_zero_pixels_input = len(image_flatten[image_flatten > 0])\n",
    "    image_rgb_flattened = image_rgb_first.flatten()\n",
    "    non_zero_pixels_result = len(image_rgb_flattened[image_rgb_flattened > 0])\n",
    "    reduction = non_zero_pixels_result / non_zero_pixels_input\n",
    "    print(f\"Reduction: {reduction}\")\n",
    "    reductions.append(reduction)\n",
    "        \n",
    "    with open(\"./results/MNIST/LIME/results.txt\", \"a\") as f:\n",
    "        f.write(f\"Image {idx}:\\n\")\n",
    "        f.write(f\"\\tPredictions: {predictions}\\n\")\n",
    "        f.write(f\"\\tConsistency: {len(set(predictions))==1}\\n\")\n",
    "        f.write(f\"\\tCorrectness: {correctness}\\n\")\n",
    "        f.write(f\"\\tGround truth: {y}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "print(f\"{np.mean(reductions)} \\pm {np.std(reductions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "X_test = torch.stack(([x for x,_ in dataset.test_dataset]))\n",
    "y_test = [y for _,y in dataset.test_dataset]\n",
    "\n",
    "explainer = shap.GradientExplainer(vgg16.model, X_test)\n",
    "\n",
    "reductions = []\n",
    "for idx in range(100):\n",
    "    x = X_test[idx].reshape(1, 3, 32, 32)\n",
    "    y = y_test[idx]\n",
    "    \n",
    "    prediction = int(torch.argmax(vgg16.model(x)))\n",
    "    predictions = [prediction]\n",
    "    correctness = (y == prediction)\n",
    "\n",
    "    X_single = x[:,0,:,:]\n",
    "    shap_values = explainer.shap_values(x)\n",
    "    shap_values = shap_values[:,0,:,:,y:y+1]\n",
    "\n",
    "    # Negative indices\n",
    "    idx_neg_shap = np.argwhere(shap_values[0,:,:,0].flatten() < 0)[:,0].tolist()\n",
    "\n",
    "    # # Plot SHAP heatmaps for all classes for a single image\n",
    "    shap.image_plot(shap_values, \n",
    "                    np.array(X_single), \n",
    "                    save=f\"./results/MNIST/SHAP/images/image-{idx}-original-iteration-0.png\")  # original SHAP\n",
    "\n",
    "    x_control_value = np.array(x.clone()[0,0,:,:].flatten())\n",
    "    x_control_value_prev = None\n",
    "    idx_neg_shap = np.argwhere(shap_values[0,:,:,0].flatten() < 0)[:,0].tolist()\n",
    "    iteration = 1\n",
    "    while True:\n",
    "        \n",
    "        # Modify the input image\n",
    "        for i in idx_neg_shap:\n",
    "            x[0,0,i//32,i%32] = 0.0\n",
    "            \n",
    "        if x_control_value_prev is not None and np.equal(x_control_value, x_control_value_prev).all():\n",
    "            break\n",
    "        \n",
    "        # Model prediction\n",
    "        prediction = int(torch.argmax(vgg16.model(x)))\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        # Compute SHAP values\n",
    "        X_single = x[:,0,:,:]\n",
    "        shap_values = explainer.shap_values(x)\n",
    "        shap_values = shap_values[:,0,:,:,y:y+1]\n",
    "\n",
    "        # Negative indices\n",
    "        idx_neg_shap = np.argwhere(shap_values[0,:,:,0].flatten() < 0)[:,0].tolist()\n",
    "        \n",
    "        # Clone the control value\n",
    "        x_control_value_prev = x_control_value.copy()\n",
    "        x_control_value = np.array(x.clone()[0,0,:,:].flatten())\n",
    "\n",
    "        # # Plot SHAP heatmaps for all classes for a single image\n",
    "        shap.image_plot(shap_values, \n",
    "                np.array(X_single), \n",
    "                save=f\"./results/MNIST/SHAP/images/image-{idx}-original-iteration-{iteration}.png\")  # original SHAP\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "    image_flatten = dataset.test_dataset[idx][0].detach().numpy().flatten()\n",
    "    non_zero_pixels_input = len(image_flatten[image_flatten > 0])\n",
    "    image_rgb_flattened = x.flatten()\n",
    "    non_zero_pixels_result = len(image_rgb_flattened[image_rgb_flattened > 0])\n",
    "    reduction = non_zero_pixels_result / non_zero_pixels_input\n",
    "    print(f\"Reduction: {reduction}\")\n",
    "    reductions.append(reduction)\n",
    "    \n",
    "        \n",
    "    with open(\"./results/MNIST/SHAP/results.txt\", \"a\") as f:\n",
    "        f.write(f\"Image {idx}:\\n\")\n",
    "        f.write(f\"\\tPredictions: {predictions}\\n\")\n",
    "        f.write(f\"\\tConsistency: {len(set(predictions))==1}\\n\")\n",
    "        f.write(f\"\\tCorrectness: {correctness}\\n\")\n",
    "        f.write(f\"\\tGround truth: {y}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "print(f\"{np.mean(reductions)} \\pm {np.std(reductions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16 as vgg16pretrained  # avoid collision with vgg16.model (our pre-trained model)\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "sys.path.append('./PyTorchRelevancePropagation/')\n",
    "from PyTorchRelevancePropagation.src.lrp import LRPModel\n",
    "\n",
    "vgg_pretrained = vgg16pretrained(weights=VGG16_Weights.DEFAULT)\n",
    "lrp_model = LRPModel(vgg_pretrained)\n",
    "\n",
    "transform_lrp = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_model = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),     # VGG16 expects 224x224\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.model.to(\"cpu\")\n",
    "img_shape = (3, 32, 32)\n",
    "\n",
    "reductions = []\n",
    "for idx in range(100):\n",
    "    \n",
    "    # Pick a random image from MNIST and pass it to the LRP model\n",
    "    print(f\"Computing fpe for Image[{idx}]...{99 - idx} left\")\n",
    "    predictions = []\n",
    "    random_index = idx\n",
    "    x, y = dataset.test_dataset[random_index]\n",
    "    x = x.view(1, *x.shape)\n",
    "    explanation = lrp_model.forward(x)\n",
    "    \n",
    "    # Compute the label for the model\n",
    "    x_model = dataset.test_dataset[random_index][0].view(1, *img_shape)\n",
    "    _, predicted = torch.max(vgg16.model(x_model).data, 1)\n",
    "    predicted = int(predicted)\n",
    "    predictions.append(predicted)\n",
    "    correctness = (y == predicted)\n",
    "\n",
    "    # Show the initial image and the explanation\n",
    "    # Create subplots: 1 row, 2 columns\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # Left: original input image\n",
    "    axes[0].imshow(x[0].permute(1, 2, 0).detach().numpy())\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Right: explanation (e.g. saliency map, activation map)\n",
    "    axes[1].imshow(explanation.detach().numpy())\n",
    "    axes[1].set_title(\"Explanation\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./results/MNIST/LRP/images/image-{idx}-original.png\")\n",
    "\n",
    "    iteration = 1\n",
    "    # Fixed point explanation\n",
    "    while True:\n",
    "        \n",
    "        # Any pixel in the explanation whose value is lower than the mean is reduced\n",
    "        x = x.view(1, 3, 32, 32)\n",
    "        for i in range(3):\n",
    "            x[:, i] *= torch.nn.functional.normalize(explanation)  # normalise the explanation and use it as a mask\n",
    "            x_model = transform_model(transforms.ToPILImage()(x[0]))\n",
    "            x_model = transforms.Resize((32, 32))(x_model)     # VGG16 expects 224x224\n",
    "        \n",
    "        # Get the new explanation\n",
    "        x = x.view(1, 3, 32, 32)\n",
    "        new_explanation = lrp_model.forward(x)\n",
    "        predicted = int(torch.max(vgg16.model(x_model.view(1, *img_shape)).data, 1)[1])\n",
    "        predictions.append(predicted)\n",
    "        \n",
    "        # # Fixed-point Explanations\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "        # Left: original input image\n",
    "        axes[0].imshow(x[0].permute(1, 2, 0).detach().numpy())\n",
    "        axes[0].set_title(\"Fixed Point Input Image\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # Right: explanation (e.g. saliency map, activation map)\n",
    "        axes[1].imshow(new_explanation.detach().numpy())\n",
    "        axes[1].set_title(\"Fixed Point Explanation\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"./results/MNIST/LRP/images/image-{idx}-recursive-iteration-{iteration}.png\")\n",
    "\n",
    "        if torch.equal(explanation, new_explanation):\n",
    "            break\n",
    "\n",
    "        # Update the previous explanation\n",
    "        explanation = new_explanation\n",
    "        iteration += 1\n",
    "        \n",
    "    image_flatten = dataset.test_dataset[random_index][0].detach().numpy().flatten()\n",
    "    non_zero_pixels_input = len(image_flatten[image_flatten > 0])\n",
    "    image_rgb_flattened = x_model.flatten()\n",
    "    non_zero_pixels_result = len(image_rgb_flattened[image_rgb_flattened > 0])\n",
    "    reduction = non_zero_pixels_result / non_zero_pixels_input\n",
    "    print(f\"Reduction: {reduction}\")\n",
    "    reductions.append(reduction)\n",
    "\n",
    "    with open(\"./results/MNIST/LRP/results.txt\", \"a\") as f:\n",
    "        f.write(f\"Image {idx}:\\n\")\n",
    "        f.write(f\"\\tPredictions: {predictions}\\n\")\n",
    "        f.write(f\"\\tConsistency: {len(set(predictions))==1}\\n\")\n",
    "        f.write(f\"\\tCorrectness: {correctness}\\n\")\n",
    "        f.write(f\"\\tGround truth: {y}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "print(f\"{np.mean(reductions)} \\pm {np.std(reductions)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
